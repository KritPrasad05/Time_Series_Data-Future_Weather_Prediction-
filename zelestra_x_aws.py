# -*- coding: utf-8 -*-
"""Zelestra_X_AWS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PTSLFfPyF7nQqldMcPJv4sRrYRU5Q9WD
"""

# Importin Libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("train.csv")
df.head()

"""#EDA

"""

df.info()

df.describe().T

df = df[(df["Datetime"] != "-80") & (df["Datetime"] != "-150")]

df["Datetime"]=pd.to_datetime(df["Datetime"], format="%d-%m-%Y %H:%M")

df["SO2_concentration"] = pd.to_numeric(df["SO2_concentration"], errors='coerce')
df["O3_concentration"] = pd.to_numeric(df["O3_concentration"], errors='coerce')
df["CO_concentration"] = pd.to_numeric(df["CO_concentration"], errors='coerce')

df.dropna(inplace=True)

df.info()

df.columns

fig,ax  = plt.subplots(figsize= (10.5,5))

plt.scatter(data = df,x="Datetime", y="Temperature");

corr = df.corr()
sns.heatmap(corr)

df= df.query("Temperature>-10 & Temperature<36")
df.shape

"""#TRAIN THE MODEL"""

import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.model_selection import TimeSeriesSplit

tss = TimeSeriesSplit(n_splits=5, test_size=24*365*1,gap=24)
df = df.sort_index()

fig, ax = plt.subplots(5,1, figsize=(10,15),
                            sharex=True)
fold=0

for train_idx, val_idx in tss.split(df):
  train = df.iloc[train_idx]
  test = df.iloc[val_idx]
  train["Temperature"].plot(ax=ax[fold],label="Training Set",title=f"Data Train/Test Split Fold {fold}",)
  test["Temperature"].plot(ax=ax[fold],label="Test Set")
  ax[fold].axvline(test.index.min(0),color="black",ls="--")
  fold += 1
plt.show()

"""# Lag Features"""

df["Datetime"]= df.set_index(df["Datetime"],drop=True,inplace=True)

df.head()

df.drop(columns = ["Datetime"],inplace=True)

def create_features(df):
  df["hour"] = df.index.hour
  df["daysofweek"] = df.index.dayofweek
  df["quater"] = df.index.quarter
  df["month"] = df.index.month
  df["year"] = df.index.year
  df["dayofyear"] = df.index.dayofyear
  df["dayofmonth"] = df.index.day
  df["weekofyear"] = df.index.isocalendar().week
  return df

def create_lag_features(df):
  target_map = df["Temperature"].to_dict()
  df["lag1"] = (df.index - pd.Timedelta('364 days')).map(target_map)
  df["lag2"] = (df.index - pd.Timedelta('728 days')).map(target_map)
  df["lag3"] = (df.index - pd.Timedelta('1092 days')).map(target_map)
  return df

df = create_lag_features(df)
df = create_features(df)
df.tail()

df.columns

tss = TimeSeriesSplit(n_splits=4, test_size=24*365*1,gap=24)
df = df.sort_index()
fold=0
preds = []
scores = []

for train_idx,val_idx in tss.split(df):
  train = df.iloc[train_idx]
  test = df.iloc[val_idx]
  Features = ['Particulate_matter', 'SO2_concentration',
              'O3_concentration', 'CO_concentration', 'NO2_concentration', 'Presure',
              'Dew_point', 'Precipitation', 'Anonymous_X1', 'Wind_speed',
              'Moisture_percent', 'lag1', 'lag2', 'lag3', 'hour',
              'daysofweek', 'quater', 'month', 'year', 'dayofyear', 'dayofmonth',
              'weekofyear']
  Target = "Temperature"
  X_train = train[Features]
  y_train = train[Target]
  X_test = test[Features]
  y_test = test[Target]

  reg = xgb.XGBRegressor(base_score = 0.5,booster = 'gbtree', early_stopping_rounds=50 ,n_estimators=700,max_depth =7,objective= 'reg:linear',learning_rate = 0.01)
  reg.fit(X_train,y_train,eval_set=[(X_train,y_train),(X_test,y_test)],verbose=100)
  y_pred = reg.predict(X_test)
  preds.append(y_pred)
  score = max(0,100-mean_squared_error(y_test,y_pred))
  scores.append(score)

scores

Features = ['Particulate_matter', 'SO2_concentration',
              'O3_concentration', 'CO_concentration', 'NO2_concentration', 'Presure',
              'Dew_point', 'Precipitation', 'Anonymous_X1', 'Wind_speed',
              'Moisture_percent', 'lag1', 'lag2', 'lag3', 'hour',
              'daysofweek', 'quater', 'month', 'year', 'dayofyear', 'dayofmonth',
              'weekofyear']
Target = "Temperature"
X_train = df[Features]
y_test = df[Target]

reg = xgb.XGBRegressor(base_score = 0.5,booster = 'gbtree', n_jobs = 1,early_stopping_rounds=50 ,n_estimators=800,max_depth = 6,objective= 'reg:squarederror',learning_rate = 0.01)
reg.fit(X_train,y_test,eval_set=[(X_train,y_test) ],verbose=100)

# Define the model and parameters
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_jobs=-1, learning_rate=0.01)

# Define parameter grid for RandomizedSearchCV
param_grid = {
    'max_depth': [5, 7, 9],
    'min_child_weight': [1, 3, 5],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9],
    'n_estimators': [1000, 1200, 1500],
}

# Search for best parameters
random_search = RandomizedSearchCV(
    xgb_model, param_distributions=param_grid, n_iter=10, scoring='neg_mean_squared_error', verbose=2
)
random_search.fit(X_train, y_test)

# Check best parameters and train final model
best_xgb_model = random_search.best_estimator_

# Predict and evaluate
y_train_pred = best_xgb_model.predict(X_train)
train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)
print(f"Training RMSE: {train_rmse}")

df_test = pd.read_csv("test.csv")
df_test.tail()

df_test.info()

def cleaning_data(df):
  if not isinstance(df, pd.DataFrame):
        raise TypeError("Input must be a pandas DataFrame")
  df["Datetime"] = pd.to_datetime(df["Datetime"], format="%d/%m/%y %H:%M", errors='coerce')
  df["SO2_concentration"] = pd.to_numeric(df["SO2_concentration"], errors='coerce')
  df["O3_concentration"] = pd.to_numeric(df["O3_concentration"], errors='coerce')
  df["CO_concentration"] = pd.to_numeric(df["CO_concentration"], errors='coerce')
  df.fillna(method="ffill",inplace=True)
  df.set_index(df["Datetime"],drop=True,inplace=True)
  df.drop(columns=["Datetime"],inplace=True)
  return df

df_test = cleaning_data(df_test)
df_test = create_features(df_test)
df_test.head()

df_test["isfuture"] = True
df["isfuture"] = False
df_future = pd.concat([df,df_test])
df_future = create_lag_features(df_future)

future = df_future.query("isfuture").copy()
future.head()

future.columns

def creating_prediction(model, df):
    # Check that input is a DataFrame
    if not isinstance(df, pd.DataFrame):
        raise TypeError("Input must be a pandas DataFrame")

    # Define the list of features required for prediction
    features = ['Particulate_matter', 'SO2_concentration',
              'O3_concentration', 'CO_concentration', 'NO2_concentration', 'Presure',
              'Dew_point', 'Precipitation', 'Anonymous_X1', 'Wind_speed',
              'Moisture_percent', 'lag1', 'lag2', 'lag3', 'hour',
              'daysofweek', 'quater', 'month', 'year', 'dayofyear', 'dayofmonth',
              'weekofyear']

    # Use the list of features correctly for model prediction
    y_test_predict = model.predict(df[features])

    # Create the submission DataFrame
    submission_dict = {
        "ID": df["ID"],
        "Temperature": y_test_predict
    }

    # Construct the DataFrame from the dictionary and set the 'ID' column as the index
    submission_file = pd.DataFrame(submission_dict)
    # Save the results to a CSV file
    submission_file.to_csv("sample_submission.csv")

creating_prediction(reg,future)

df_sub= pd.read_csv("sample_submission.csv")
df_sub

df_sub.index

